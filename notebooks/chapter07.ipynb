{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFHvPBvbxLTTKlU7tZN/80"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 第7章: 単語ベクトル\n","\n","単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して，以下の処理を行うプログラムを作成せよ．"],"metadata":{"id":"iBPFEkMS3ncN"}},{"cell_type":"code","source":[],"metadata":{"id":"1YQpfygsrJ49","executionInfo":{"status":"ok","timestamp":1669646666592,"user_tz":-540,"elapsed":16,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 60. 単語ベクトルの読み込みと表示\n","---\n","Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"],"metadata":{"id":"YtKpfXZ02sUM"}},{"cell_type":"code","source":[],"metadata":{"id":"Gt_x3O-x3UqZ","executionInfo":{"status":"ok","timestamp":1669646666593,"user_tz":-540,"elapsed":16,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 61. 単語の類似度\n","---\n","“United States”と”U.S.”のコサイン類似度を計算せよ．"],"metadata":{"id":"drBaISP1tZhw"}},{"cell_type":"code","source":[],"metadata":{"id":"GE8TYCzTt4Vo","executionInfo":{"status":"ok","timestamp":1669646666595,"user_tz":-540,"elapsed":17,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 62. 類似度の高い単語10件\n","---\n","“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"],"metadata":{"id":"9XrFU7FC4wPJ"}},{"cell_type":"code","source":[],"metadata":{"id":"-rtmr1fo5FxY","executionInfo":{"status":"ok","timestamp":1669646666596,"user_tz":-540,"elapsed":18,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 63. 加法構成性によるアナロジー\n","---\n","“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"],"metadata":{"id":"qM8Oh2xF5gv6"}},{"cell_type":"code","source":[],"metadata":{"id":"eeiA2Jxm6Ks3","executionInfo":{"status":"ok","timestamp":1669646666596,"user_tz":-540,"elapsed":17,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 64. アナロジーデータでの実験\n","---\n","[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"],"metadata":{"id":"kU1t8DcC5RcH"}},{"cell_type":"code","source":[],"metadata":{"id":"tuPL0www6KSe","executionInfo":{"status":"ok","timestamp":1669646666598,"user_tz":-540,"elapsed":18,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 65. アナロジータスクでの正解率\n","---\n","64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"],"metadata":{"id":"lLIUhBth5X3p"}},{"cell_type":"code","source":[],"metadata":{"id":"eEhtUeFm6JyA","executionInfo":{"status":"ok","timestamp":1669646666599,"user_tz":-540,"elapsed":19,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 66. WordSimilarity-353での評価\n","---\n","[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"],"metadata":{"id":"GOHBNstV5m0k"}},{"cell_type":"code","source":[],"metadata":{"id":"OfAaaw4C6JLQ","executionInfo":{"status":"ok","timestamp":1669646666599,"user_tz":-540,"elapsed":18,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 67. k-meansクラスタリング\n","---\n","国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"],"metadata":{"id":"ngMBE-7r5tZ8"}},{"cell_type":"code","source":[],"metadata":{"id":"HzReSHmz6Isz","executionInfo":{"status":"ok","timestamp":1669646666600,"user_tz":-540,"elapsed":19,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 68. Ward法によるクラスタリング\n","---\n","国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"],"metadata":{"id":"6PJaiRui6AD_"}},{"cell_type":"code","source":[],"metadata":{"id":"jNl5dYe2G-uF","executionInfo":{"status":"ok","timestamp":1669646666600,"user_tz":-540,"elapsed":18,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 69. t-SNEによる可視化\n","---\n","ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"],"metadata":{"id":"Tclp37O76Gu2"}},{"cell_type":"code","source":[],"metadata":{"id":"KUucxgt1__Db","executionInfo":{"status":"ok","timestamp":1669646666600,"user_tz":-540,"elapsed":18,"user":{"displayName":"Haruya Suzuki","userId":"04639647374528570616"}}},"execution_count":null,"outputs":[]}]}