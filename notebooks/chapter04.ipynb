{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 第4章: 形態素解析\n",
        "\n",
        "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をMeCabを使って形態素解析し，その結果をneko.txt.mecabというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．\n",
        "\n",
        "なお，問題37, 38, 39は[matplotlib](http://matplotlib.org/)もしくは[Gnuplot](http://www.gnuplot.info/)を用いるとよい．\n"
      ],
      "metadata": {
        "id": "iBPFEkMS3ncN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp100.github.io/data/neko.txt"
      ],
      "metadata": {
        "id": "1YQpfygsrJ49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75bb522-8a38-42a2-9b55-e05f5f97b8d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-04 19:31:54--  https://nlp100.github.io/data/neko.txt\n",
            "Resolving nlp100.github.io (nlp100.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to nlp100.github.io (nlp100.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 975789 (953K) [text/plain]\n",
            "Saving to: ‘neko.txt’\n",
            "\n",
            "\rneko.txt              0%[                    ]       0  --.-KB/s               \rneko.txt            100%[===================>] 952.92K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-12-04 19:31:54 (25.2 MB/s) - ‘neko.txt’ saved [975789/975789]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install mecab libmecab-dev mecab-ipadic-utf8\n",
        "!mecab neko.txt -o neko.txt.mecab"
      ],
      "metadata": {
        "id": "MluPbGAgI7jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92b2716-4cb1-4422-c535-66554d4040ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libmecab-dev is already the newest version (0.996-5).\n",
            "mecab is already the newest version (0.996-5).\n",
            "mecab-ipadic-utf8 is already the newest version (2.7.0-20070801+main-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 30. 形態素解析結果の読み込み\n",
        "---\n",
        "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ．ただし，各形態素は表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をキーとするマッピング型に格納し，1文を形態素（マッピング型）のリストとして表現せよ．第4章の残りの問題では，ここで作ったプログラムを活用せよ．"
      ],
      "metadata": {
        "id": "YtKpfXZ02sUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "sentence = []\n",
        "with open(\"./neko.txt.mecab\") as f:\n",
        "    for line in f:\n",
        "        if line == \"EOS\\n\":\n",
        "            sentences.append(sentence)\n",
        "            sentence = []\n",
        "            break\n",
        "        else:\n",
        "            surface, feature = line.split(\"\\t\")\n",
        "            features = feature.split(\",\")\n",
        "            morpheme_dict = {\n",
        "                \"surface\": surface,\n",
        "                \"base\": features[6],\n",
        "                \"pos\": features[0],\n",
        "                \"pos1\": features[1],\n",
        "            }\n",
        "            sentence.append(morpheme_dict)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "Gt_x3O-x3UqZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "17c9c2e4-b89c-4def-a180-0b4f918bfb77"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d7ccb9c93294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msurface\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             morpheme_dict = {\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 31. 動詞Permalink\n",
        "---\n",
        "動詞の表層形をすべて抽出せよ．"
      ],
      "metadata": {
        "id": "drBaISP1tZhw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GE8TYCzTt4Vo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 62. 類似度の高い単語10件\n",
        "---\n",
        "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
      ],
      "metadata": {
        "id": "9XrFU7FC4wPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rtmr1fo5FxY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 63. 加法構成性によるアナロジー\n",
        "---\n",
        "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
      ],
      "metadata": {
        "id": "qM8Oh2xF5gv6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeiA2Jxm6Ks3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 64. アナロジーデータでの実験\n",
        "---\n",
        "[単語アナロジーの評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
      ],
      "metadata": {
        "id": "kU1t8DcC5RcH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuPL0www6KSe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 65. アナロジータスクでの正解率\n",
        "---\n",
        "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
      ],
      "metadata": {
        "id": "lLIUhBth5X3p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEhtUeFm6JyA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 66. WordSimilarity-353での評価\n",
        "---\n",
        "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
      ],
      "metadata": {
        "id": "GOHBNstV5m0k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OfAaaw4C6JLQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 67. k-meansクラスタリング\n",
        "---\n",
        "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
      ],
      "metadata": {
        "id": "ngMBE-7r5tZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzReSHmz6Isz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 68. Ward法によるクラスタリング\n",
        "---\n",
        "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
      ],
      "metadata": {
        "id": "6PJaiRui6AD_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNl5dYe2G-uF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 69. t-SNEによる可視化\n",
        "---\n",
        "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
      ],
      "metadata": {
        "id": "Tclp37O76Gu2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUucxgt1__Db"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}